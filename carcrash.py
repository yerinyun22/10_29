# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
import plotly.express as px
from datetime import datetime
from math import radians, sin, cos, sqrt, atan2

st.set_page_config(page_title="ì‚¬ê³ ë‹¤ë°œì§€ì—­ ì•ˆì „ì§€ë„(ê·¼ì‚¬)", layout="wide")
url = "https://drive.google.com/file/d/1c3ULCZImSX4ns8F9cIE2wVsy8Avup8bu/view?usp=sharing"
df = pd.read_csv(url, encoding="cp949")  # ë˜ëŠ” cp949

# -------------------------
# ìœ í‹¸: ê±°ë¦¬(ìœ„ë„/ê²½ë„) ê³„ì‚° â€” Haversine (km)
# -------------------------
def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0  # km
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

# -------------------------
# ë°ì´í„° ë¡œë“œ
# -------------------------
@st.cache_data
def load_data(path="í•œêµ­ë„ë¡œêµí†µê³µë‹¨_êµí†µì‚¬ê³ ë‹¤ë°œì§€ì—­_20250924.csv"):
    # try both utf-8 and cp949
    try:
        df = pd.read_csv(path, encoding="utf-8")
    except Exception:
        df = pd.read_csv(path, encoding="cp949")
    # í‘œì¤€í™” ì»¬ëŸ¼ëª…(ìˆìœ¼ë©´)
    df.columns = [c.strip() for c in df.columns]
    return df

data = load_data()

st.title("ğŸ›¡ï¸ ì‚¬ê³ ë‹¤ë°œì§€ì—­ ì•ˆì „ì§€ë„ â€” ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ & ìœ„í—˜ ë ˆì´ì–´")
st.markdown(
    "ì´ ì•±ì€ í•œêµ­ë„ë¡œêµí†µê³µë‹¨ì˜ ì‚¬ê³ ë‹¤ë°œì§€ì—­ CSVë¥¼ ì‚¬ìš©í•´ **ì‚¬ê³  ìœ„ì¹˜ í‘œì¶œ**, **íˆíŠ¸ë§µ/í´ëŸ¬ìŠ¤í„°**, "
    "**í•„í„°/ê²€ìƒ‰**, ê·¸ë¦¬ê³  **ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ(ë°ì´í„° ê¸°ë°˜)** ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n"
    ":warning: ì‹¤ì œ ë‚´ë¹„ê²Œì´ì…˜(ë„ë¡œ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ê²½ë¡œ)ì€ ì™¸ë¶€ ë¼ìš°íŒ… APIê°€ í•„ìš”í•©ë‹ˆë‹¤. "
    "ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë§Œìœ¼ë¡œ ê°€ëŠ¥í•œ ê·¼ì‚¬(ìœ„í—˜ íšŒí”¼) ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
)

# -------------------------
# ê¸°ë³¸ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬ (ìœ ì € ìš”êµ¬ì— ë§ì¶° UI í™œì„±í™”)
# -------------------------
has_latlon = {"ìœ„ë„", "ê²½ë„"}.issubset(set(data.columns))
has_year = "ì‚¬ê³ ì—°ë„" in data.columns or "ì—°ë„" in data.columns
# í‘œì¤€í™”: ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ë„ ì»¬ëŸ¼ëª…
year_col = "ì‚¬ê³ ì—°ë„" if "ì‚¬ê³ ì—°ë„" in data.columns else ("ì—°ë„" if "ì—°ë„" in data.columns else None)
type_col = "ì‚¬ê³ ìœ í˜•êµ¬ë¶„" if "ì‚¬ê³ ìœ í˜•êµ¬ë¶„" in data.columns else None
severity_related_cols = set(["ì‚¬ë§ììˆ˜", "ì¤‘ìƒììˆ˜", "ê²½ìƒììˆ˜", "ì‚¬ê³ ê±´ìˆ˜", "ì‚¬ìƒììˆ˜"]) & set(data.columns)

# -------------------------
# ì‚¬ì´ë“œë°”: í•„í„° & ê²€ìƒ‰
# -------------------------
st.sidebar.header("ğŸ” í•„í„° Â· ê²€ìƒ‰ Â· ì•ˆì „ê²½ë¡œ")

# ì—°ë„ í•„í„° (ìˆìœ¼ë©´)
if year_col:
    years = sorted(data[year_col].dropna().unique().astype(int))
    sel_year = st.sidebar.slider("ì—°ë„ ì„ íƒ", min_value=int(min(years)), max_value=int(max(years)), value=int(max(years)))
else:
    sel_year = None

# ì‚¬ê³ ìœ í˜• í•„í„° (ìˆìœ¼ë©´)
if type_col:
    types = sorted(data[type_col].dropna().unique())
    sel_types = st.sidebar.multiselect("ì‚¬ê³ ìœ í˜• í•„í„°", options=types, default=types)
else:
    sel_types = None

# ì‚¬ê³ ì›ì¸(ì»¬ëŸ¼ ì—†ìœ¼ë©´ ë¹„í™œì„±)
possible_cause_cols = [c for c in data.columns if "ì›ì¸" in c or "ì‚¬ê³ ì›ì¸" in c or "ë°œìƒì›ì¸" in c]
cause_col = possible_cause_cols[0] if possible_cause_cols else None
if cause_col:
    causes = sorted(data[cause_col].dropna().unique())
    sel_causes = st.sidebar.multiselect("ì‚¬ê³ ì›ì¸ í•„í„°", options=causes, default=causes)
else:
    sel_causes = None

# ë‚ ì§œ í•„í„°(ë°ì´í„°ê¸°ì¤€ì¼ì ë“±)
date_col = None
for c in data.columns:
    if "ì¼ì" in c or "date" in c.lower() or "ë‚ ì§œ" in c:
        date_col = c
        break

if date_col:
    # try parse dates
    try:
        data[date_col + "_parsed"] = pd.to_datetime(data[date_col], errors="coerce")
        min_d = data[date_col + "_parsed"].min()
        max_d = data[date_col + "_parsed"].max()
        sel_dates = st.sidebar.date_input("ê¸°ê°„ í•„í„°", value=(max_d.date(), max_d.date()), min_value=min_d.date(), max_value=max_d.date())
        # sel_dates is tuple(start,end)
    except Exception:
        date_col = None

# ê²€ìƒ‰(ì§€ì—­ëª…/ìœ„ì¹˜ëª…)
search_text = st.sidebar.text_input("ê²€ìƒ‰(ì§€ì—­ëª… / ìœ„ì¹˜ì½”ë“œ / ìœ„ì¹˜ëª…) â€” ë¹ˆì¹¸=ì „ì²´", "")

# ì•ˆì „ê²½ë¡œ ì…ë ¥ (ì¶œë°œ/ë„ì°© ì„ íƒ) â€” ë°ì´í„°ì— ìˆëŠ” ìœ„ì¹˜ëª… ëª©ë¡ ì‚¬ìš©
loc_name_col = None
for c in ["ì‚¬ê³ ì§€ì—­ìœ„ì¹˜ëª…", "ì‚¬ê³ ë‹¤ë°œì§€ì—­ì‹œë„ì‹œêµ°êµ¬", "ìœ„ì¹˜ì½”ë“œ"]:
    if c in data.columns:
        loc_name_col = c
        break

loc_options = data[loc_name_col].astype(str).unique().tolist() if loc_name_col else []

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸš— ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ(ë°ì´í„° ê¸°ë°˜)")
start_sel = st.sidebar.selectbox("ì¶œë°œì§€ (ë°ì´í„° ìœ„ì¹˜ ì¤‘ ì„ íƒ)", options=loc_options, index=0 if loc_options else -1)
end_sel = st.sidebar.selectbox("ë„ì°©ì§€ (ë°ì´í„° ìœ„ì¹˜ ì¤‘ ì„ íƒ)", options=loc_options, index=len(loc_options)-1 if loc_options else -1)
# í›„ë³´ ìˆ˜, ìƒ˜í”Œ í¬ì¸íŠ¸ ìˆ˜
candidate_count = st.sidebar.slider("ê²½ë¡œ í›„ë³´ ìˆ˜", 3, 9, 5)
samples_per_candidate = st.sidebar.slider("ê²½ë¡œ ìƒ˜í”Œ ì  ê°œìˆ˜(ì •ë°€ë„)", 10, 80, 30)
avoid_radius_km = st.sidebar.slider("ìœ„í—˜ ê°€ì¤‘ ë°˜ê²½ (km)", 0.2, 3.0, 0.7)

# ìœ„í—˜ êµ¬ê°„ ê²½ê³ : í˜„ì¬ ìœ„ì¹˜(ìœ„ë„/ê²½ë„) ìˆ˜ë™ ì…ë ¥(ë˜ëŠ” ì„ íƒ)
st.sidebar.markdown("---")
st.sidebar.subheader("âš ï¸ ìœ„í—˜ êµ¬ê°„ ê²½ê³  (ì„ì˜ ìœ„ì¹˜)")
alert_lat = st.sidebar.number_input("ìœ„ë„ ì…ë ¥", value=float(data["ìœ„ë„"].mean()) if has_latlon else 37.56)
alert_lon = st.sidebar.number_input("ê²½ë„ ì…ë ¥", value=float(data["ê²½ë„"].mean()) if has_latlon else 126.97)
alert_radius_km = st.sidebar.slider("ê²½ê³  ë°˜ê²½ (km)", 0.1, 5.0, 0.5)
# -------------------------
# ë°ì´í„° í•„í„°ë§ ì ìš©
# -------------------------
df = data.copy()

if sel_year and year_col:
    df = df[df[year_col] == sel_year]

if sel_types and type_col:
    df = df[df[type_col].isin(sel_types)]

if sel_causes and cause_col:
    df = df[df[cause_col].isin(sel_causes)]

if date_col:
    start_d, end_d = sel_dates
    mask = (df[date_col + "_parsed"].dt.date >= start_d) & (df[date_col + "_parsed"].dt.date <= end_d)
    df = df[mask]

if search_text:
    search_text = search_text.strip()
    # search in possible text columns
    text_cols = [c for c in df.columns if df[c].dtype == object]
    mask = pd.Series(False, index=df.index)
    for c in text_cols:
        mask = mask | df[c].astype(str).str.contains(search_text, case=False, na=False)
    df = df[mask]

# -------------------------
# ìƒ‰ìƒ/ì‹¬ê°ë„ ê³„ì‚°(ê°„ë‹¨ ê°€ì¤‘ì¹˜)
# -------------------------
# ìš°ì„  'ì‚¬ë§ììˆ˜' ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ 'ì‚¬ìƒììˆ˜'ë‚˜ 'ì¤‘ìƒììˆ˜'ë¡œ ëŒ€ì²´
def severity_score(row):
    score = 0.0
    if "ì‚¬ë§ììˆ˜" in row.index:
        score += 10.0 * (row.get("ì‚¬ë§ììˆ˜", 0) or 0)
    if "ì¤‘ìƒììˆ˜" in row.index:
        score += 3.0 * (row.get("ì¤‘ìƒììˆ˜", 0) or 0)
    if "ê²½ìƒììˆ˜" in row.index:
        score += 1.0 * (row.get("ê²½ìƒììˆ˜", 0) or 0)
    # ì‚¬ê³ ê±´ìˆ˜ë„ ê°€ë¯¸
    if "ì‚¬ê³ ê±´ìˆ˜" in row.index:
        score += 0.5 * (row.get("ì‚¬ê³ ê±´ìˆ˜", 0) or 0)
    return score

if len(df) > 0:
    df["sev_score"] = df.apply(severity_score, axis=1)
else:
    df["sev_score"] = []

# ìƒ‰ìƒ ë§¤í•‘ í•¨ìˆ˜
def severity_to_color(s):
    # s >= 10 : ì•„ì£¼ ìœ„í—˜(ì§„í•œ ë¹¨ê°•), 5~10: ë¹¨ê°•, 2~5: ì£¼í™©, <2: ë…¸ë‘/ì—°í•œíšŒìƒ‰
    if s >= 10:
        return [180, 0, 0, 200]
    elif s >= 5:
        return [230, 40, 40, 180]
    elif s >= 2:
        return [255, 140, 0, 150]
    elif s > 0:
        return [255, 210, 0, 130]
    else:
        return [150, 150, 150, 90]

if len(df) > 0:
    df["color"] = df["sev_score"].apply(severity_to_color)
else:
    df["color"] = []

# -------------------------
# ë©”ì¸ ë ˆì´ì•„ì›ƒ: ì§€ë„ + ì»¨íŠ¸ë¡¤
# -------------------------
st.subheader("ì§€ë„ Â· íˆíŠ¸ë§µ Â· í´ëŸ¬ìŠ¤í„° Â· ë§ˆì»¤")
if not has_latlon:
    st.error("ë°ì´í„°ì— 'ìœ„ë„' / 'ê²½ë„' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ íŒŒì¼ì— í•´ë‹¹ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    # view ì´ˆê¸°ê°’: ëŒ€í•œë¯¼êµ­(ë˜ëŠ” ë°ì´í„° ì¤‘ì‹¬)
    center_lat = float(df["ìœ„ë„"].mean()) if not np.isnan(df["ìœ„ë„"].mean()) else 37.56
    center_lon = float(df["ê²½ë„"].mean()) if not np.isnan(df["ê²½ë„"].mean()) else 126.97

    # í”¼ì²˜: Scatter(ë§ˆì»¤), Heatmap, Hexagon(í´ëŸ¬ìŠ¤í„°), Line(ì•ˆì „ê²½ë¡œ)
    layers = []

    # íˆíŠ¸ë§µ ë ˆì´ì–´
    heat_layer = pdk.Layer(
        "HeatmapLayer",
        data=df,
        get_position=["ê²½ë„", "ìœ„ë„"],
        aggregation="SUM",
        weight="sev_score" if "sev_score" in df.columns else None,
        radiusPixels=60,
    )
    layers.append(heat_layer)

    # Hexagon í´ëŸ¬ìŠ¤í„° ë ˆì´ì–´ (ì§‘ì¤‘ë„ í‘œì‹œ)
    hex_layer = pdk.Layer(
        "HexagonLayer",
        data=df,
        get_position=["ê²½ë„", "ìœ„ë„"],
        radius=200,  # meters (approx)
        elevation_scale=50,
        elevation_range=[0, 3000],
        pickable=True,
        extruded=True,
    )
    layers.append(hex_layer)

    # ì‚¬ê³  ë§ˆì»¤(ì‹¬ê°ë„ ìƒ‰ìƒ)
    scatter = pdk.Layer(
        "ScatterplotLayer",
        data=df,
        get_position=["ê²½ë„", "ìœ„ë„"],
        get_color="color",
        get_radius=60,
        pickable=True,
    )
    layers.append(scatter)

    view_state = pdk.ViewState(latitude=center_lat, longitude=center_lon, zoom=6, pitch=0)

    tooltip = {
        "html": "<b>{ì‚¬ê³ ì§€ì—­ìœ„ì¹˜ëª…}</b><br/>ì‚¬ê³ ê±´ìˆ˜: {ì‚¬ê³ ê±´ìˆ˜} / ì‚¬ìƒì: {ì‚¬ìƒììˆ˜} / ì‹¬ê°ë„:{sev_score}",
        "style": {"color": "white"}
    }

    deck = pdk.Deck(
        map_style="mapbox://styles/mapbox/light-v9",
        initial_view_state=view_state,
        layers=layers,
        tooltip=tooltip
    )

    st.pydeck_chart(deck, use_container_width=True)

# -------------------------
# êµ¬ë³„/ìœ í˜•ë³„ í†µê³„ (plotly)
# -------------------------
st.subheader("í†µê³„ Â· í•„í„°ëœ ê²°ê³¼ ìš”ì•½")
if "ì‚¬ê³ ë‹¤ë°œì§€ì—­ì‹œë„ì‹œêµ°êµ¬" in df.columns and "ì‚¬ê³ ê±´ìˆ˜" in df.columns:
    by_dist = df.groupby("ì‚¬ê³ ë‹¤ë°œì§€ì—­ì‹œë„ì‹œêµ°êµ¬")["ì‚¬ê³ ê±´ìˆ˜"].sum().sort_values(ascending=False).reset_index()
    fig = px.bar(by_dist.head(15), x="ì‚¬ê³ ë‹¤ë°œì§€ì—­ì‹œë„ì‹œêµ°êµ¬", y="ì‚¬ê³ ê±´ìˆ˜", title="êµ¬ë³„ ì‚¬ê³ ê±´ìˆ˜ Top 15")
    st.plotly_chart(fig, use_container_width=True)
else:
    st.info("êµ¬ë³„ ì‚¬ê³ ê±´ìˆ˜ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ 'ì‚¬ê³ ë‹¤ë°œì§€ì—­ì‹œë„ì‹œêµ°êµ¬'ì™€ 'ì‚¬ê³ ê±´ìˆ˜' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

if type_col and "ì‚¬ê³ ê±´ìˆ˜" in df.columns:
    by_type = df.groupby(type_col)["ì‚¬ê³ ê±´ìˆ˜"].sum().sort_values(ascending=False).reset_index()
    fig2 = px.pie(by_type, values="ì‚¬ê³ ê±´ìˆ˜", names=type_col, title="ì‚¬ê³ ìœ í˜•ë³„ ë¹„ìœ¨")
    st.plotly_chart(fig2, use_container_width=True)

# -------------------------
# ìœ„í—˜ êµ¬ê°„ ê²½ê³  (ë‹¨ìˆœ ê·¼ì ‘ ê¸°ë°˜)
# -------------------------
st.subheader("ìœ„í—˜ êµ¬ê°„ ê²½ê³  (ì…ë ¥ ì¢Œí‘œ ê¸°ì¤€)")
if has_latlon:
    # ê±°ë¦¬ ê³„ì‚°: alert point ì™€ ê° ì‚¬ê³ ì§€ì  ê°„ ê±°ë¦¬ ê³„ì‚°
    df["dist_to_alert_km"] = df.apply(lambda r: haversine(alert_lat, alert_lon, float(r["ìœ„ë„"]), float(r["ê²½ë„"])), axis=1)
    nearby = df[df["dist_to_alert_km"] <= alert_radius_km]
    st.write(f"ì„ íƒ ë°˜ê²½ {alert_radius_km} km ë‚´ ì‚¬ê³ ë‹¤ë°œì§€ ìˆ˜: {len(nearby)}")
    if len(nearby) > 0:
        # ìš”ì•½
        st.dataframe(nearby[["ì‚¬ê³ ì§€ì—­ìœ„ì¹˜ëª…", "ì‚¬ê³ ê±´ìˆ˜", "ì‚¬ìƒììˆ˜", "dist_to_alert_km"]].sort_values("dist_to_alert_km").head(10))
    else:
        st.info("ì„ íƒí•œ ë°˜ê²½ ë‚´ ìœ„í—˜ì§€ì ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ìœ„ì¹˜ ë°ì´í„°(ìœ„ë„/ê²½ë„)ê°€ ì—†ì–´ ìœ„í—˜êµ¬ê°„ ê²½ê³ ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# -------------------------
# ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ ìƒì„± ë° ì‹œê°í™”
# -------------------------
st.subheader("ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ (ë°ì´í„° ê¸°ë°˜ í›„ë³´ ìƒì„± ë° ìœ„í—˜ë„ ë¹„êµ)")

def get_coords_for_loc(name):
    # loc_name_colì„ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ìœ„ì¹˜ì˜ í‰ê·  ì¢Œí‘œ ë°˜í™˜
    subset = data[data[loc_name_col].astype(str) == str(name)]
    if len(subset) == 0:
        return None
    return float(subset["ìœ„ë„"].mean()), float(subset["ê²½ë„"].mean())

start_coord = get_coords_for_loc(start_sel) if loc_name_col and start_sel else None
end_coord = get_coords_for_loc(end_sel) if loc_name_col and end_sel else None

if (start_coord is None) or (end_coord is None):
    st.info("ì¶œë°œì§€/ë„ì°©ì§€ì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ìœ„ì¹˜ ì„ íƒì´ í•„ìš”).")
else:
    st.write("ì¶œë°œ:", start_sel, "ìœ„ì¹˜:", start_coord)
    st.write("ë„ì°©:", end_sel, "ìœ„ì¹˜:", end_coord)

    # í›„ë³´ ê²½ë¡œ ìƒì„±: ì›ë˜ ì§ì„  ê²½ë¡œ + ì—¬ëŸ¬ ê°ë„ offsetìœ¼ë¡œ ì¡°ê¸ˆì”© ìš°íšŒí•˜ëŠ” í›„ë³´ë“¤
    def sample_line(lat1, lon1, lat2, lon2, n):
        lats = np.linspace(lat1, lat2, n)
        lons = np.linspace(lon1, lon2, n)
        return list(zip(lats, lons))

    def offset_candidate(lat1, lon1, lat2, lon2, offset_deg):
        # offset_deg : degree to rotate around midpoint in lat/lon space (approx)
        mid_lat = (lat1 + lat2) / 2
        mid_lon = (lon1 + lon2) / 2
        # vector from mid to endpoints
        v1 = np.array([lat1 - mid_lat, lon1 - mid_lon])
        v2 = np.array([lat2 - mid_lat, lon2 - mid_lon])
        # rotate both vectors by offset_deg
        theta = np.radians(offset_deg)
        R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])
        nv1 = R.dot(v1)
        nv2 = R.dot(v2)
        # reconstruct endpoints and sample between them
        nlat1, nlon1 = mid_lat + nv1[0], mid_lon + nv1[1]
        nlat2, nlon2 = mid_lat + nv2[0], mid_lon + nv2[1]
        return sample_line(nlat1, nlon1, nlat2, nlon2, samples_per_candidate)

    # í‰ê°€ í•¨ìˆ˜: ê²½ë¡œì˜ ìœ„í—˜ ì ìˆ˜ = ê° ìƒ˜í”Œ í¬ì¸íŠ¸ì—ì„œ ì£¼ë³€ ì‚¬ê³ ì˜ ê±°ë¦¬ ê¸°ë°˜ ìœ„í—˜ í•©
    def path_risk_score(path_points, accidents_df, radius_km=avoid_radius_km):
        total = 0.0
        # for each sample point, sum gaussian-like weight from accidents
        for (plat, plon) in path_points:
            # compute distances vectorized
            dists = haversine_vectorized(plat, plon, accidents_df["ìœ„ë„"].values, accidents_df["ê²½ë„"].values)
            # only consider within radius; weight = sev_score * exp(-(d/r)^2)
            within_mask = dists <= (radius_km * 3)  # consider up to 3*r
            if not np.any(within_mask):
                continue
            rel = dists[within_mask] / (radius_km + 1e-6)
            weights = np.exp(- (rel**2))
            sev = accidents_df["sev_score"].values[within_mask]
            total += np.sum(weights * (sev + 0.1))  # avoid zero
        return total

    # vectorized haversine over numpy arrays
    def haversine_vectorized(lat1, lon1, lat_arr, lon_arr):
        R = 6371.0
        lat1r = np.radians(lat1)
        lon1r = np.radians(lon1)
        lat2r = np.radians(lat_arr)
        lon2r = np.radians(lon_arr)
        dlat = lat2r - lat1r
        dlon = lon2r - lon1r
        a = np.sin(dlat / 2) ** 2 + np.cos(lat1r) * np.cos(lat2r) * np.sin(dlon / 2) ** 2
        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
        return R * c

    # generate candidate offsets: symmetric offsets between -max to +max degrees
    max_offset = 30  # degrees rotation max
    offsets = np.linspace(-max_offset, max_offset, candidate_count)
    candidates = []
    for off in offsets:
        pts = offset_candidate(start_coord[0], start_coord[1], end_coord[0], end_coord[1], off)
        candidates.append(pts)

    # compute risk for each candidate using current filtered df (we consider all accidents in df)
    candidate_scores = []
    for pts in candidates:
        score = path_risk_score(pts, df, radius_km=avoid_radius_km)
        candidate_scores.append(score)

    # choose best (min risk)
    best_idx = int(np.argmin(candidate_scores))
    best_path = candidates[best_idx]

    st.write(f"ìƒì„±ëœ í›„ë³´ {len(candidates)}ê°œ ì¤‘ ìœ„í—˜ë„ ìµœì € ê²½ë¡œ: í›„ë³´ #{best_idx+1} (score={candidate_scores[best_idx]:.2f})")

    # ì‹œê°í™”: ìœ„ ì§€ë„ ìœ„ì— ìµœì €ê²½ë¡œ ì„ ìœ¼ë¡œ í‘œì‹œ
    if has_latlon:
        line_layer = pdk.Layer(
            "LineLayer",
            data=[{"path": [(p[1], p[0]) for p in best_path], "name": "best"}],
            get_path="path",
            get_width=6,
            get_color=[30, 144, 255],
        )

        deck_with_path = pdk.Deck(
            map_style="mapbox://styles/mapbox/light-v9",
            initial_view_state=pdk.ViewState(latitude=center_lat, longitude=center_lon, zoom=7),
            layers=[heat_layer, hex_layer, scatter, line_layer],
            tooltip=tooltip
        )
        st.pydeck_chart(deck_with_path, use_container_width=True)

    # í‘œë¡œ í›„ë³´ ë¹„êµ
    comp_df = pd.DataFrame({"candidate": list(range(1, len(candidate_scores) + 1)), "offset_deg": offsets, "risk_score": candidate_scores})
    st.table(comp_df.sort_values("risk_score").reset_index(drop=True))

# -------------------------
# ë§ˆë¬´ë¦¬: ì°¸ê³  ë° í•œê³„
# -------------------------
st.markdown("---")
st.subheader("ì°¸ê³  ë° í•œê³„")
st.markdown(
    """
- ì•ˆì „ê²½ë¡œëŠ” **ë„ë¡œ ë„¤íŠ¸ì›Œí¬ê°€ ì•„ë‹Œ, ì‚¬ê³  ë°ì´í„° ë¶„í¬ ê¸°ë°˜ì˜ ê·¼ì‚¬(ìš°íšŒ í›„ë³´ ìƒì„±)** ì…ë‹ˆë‹¤.
  ì‹¤ì œ ë„ë¡œ ê¸°ë°˜ ê²½ë¡œ ìµœì í™”(ì‹¤ì‹œê°„ êµí†µ, ë„ë¡œ ì°¨ë¡œ ë“±)ëŠ” **ì™¸ë¶€ ë¼ìš°íŒ… API**(OSRM, Google Directions ë“±) + êµí†µ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- ì‚¬ê³ ì›ì¸/ì •í™• ë°œìƒì‹œê° ë“± ë°ì´í„° ì»¬ëŸ¼ì´ íŒŒì¼ì— ì—†ì„ ê²½ìš° ê´€ë ¨ í•„í„°ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.
- ì‹¤ì‚¬ìš© ìˆ˜ì¤€ì˜ 'ì‹¤ì‹œê°„ ìœ„í—˜ ì•Œë¦¼' ê¸°ëŠ¥ì„ ë§Œë“¤ë ¤ë©´ ëª¨ë°”ì¼ ìœ„ì¹˜ ì ‘ê·¼ ë° ë°±ì—”ë“œ(ì„œë²„) í˜•íƒœì˜ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.
"""
)

