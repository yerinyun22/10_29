import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
import plotly.express as px
from datetime import datetime
from math import radians, sin, cos, sqrt, atan2
import gdown  # Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ

st.set_page_config(page_title="ì‚¬ê³ ë‹¤ë°œì§€ì—­ ì•ˆì „ì§€ë„(ê·¼ì‚¬)", layout="wide")

# -------------------------
# ìœ í‹¸: ê±°ë¦¬(ìœ„ë„/ê²½ë„) ê³„ì‚° â€” Haversine (km)
# -------------------------
def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0  # km
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

def haversine_vectorized(lat1, lon1, lat_arr, lon_arr):
    R = 6371.0
    lat1r = np.radians(lat1)
    lon1r = np.radians(lon1)
    lat2r = np.radians(lat_arr)
    lon2r = np.radians(lon_arr)
    dlat = lat2r - lat1r
    dlon = lon2r - lon1r
    a = np.sin(dlat / 2) ** 2 + np.cos(lat1r) * np.cos(lat2r) * np.sin(dlon / 2) ** 2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    return R * c

# -------------------------
# ë°ì´í„° ë¡œë“œ (Google Drive)
# -------------------------
@st.cache_data
def load_data_from_drive(drive_url):
    file_id = drive_url.split("/")[5]
    download_url = f"https://drive.google.com/uc?id={file_id}"
    local_path = "accidents.csv"
    gdown.download(download_url, local_path, quiet=True)

    encodings = ["utf-8", "cp949", "euc-kr"]
    for enc in encodings:
        try:
            df = pd.read_csv(local_path, encoding=enc, on_bad_lines="skip")
            df.columns = [c.strip() for c in df.columns]
            print(f"CSV ë¡œë“œ ì„±ê³µ: encoding={enc}")
            return df
        except Exception as e:
            print(f"CSV ë¡œë“œ ì‹¤íŒ¨({enc}): {e}")
    raise ValueError("CSVë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì½”ë”© ë¬¸ì œ í™•ì¸ í•„ìš”")

drive_link = "https://drive.google.com/file/d/1c3ULCZImSX4ns8F9cIE2wVsy8Avup8bu/view?usp=sharing"
data = load_data_from_drive(drive_link)

# -------------------------
# Streamlit UI
# -------------------------
st.title("ğŸ›¡ï¸ ì‚¬ê³ ë‹¤ë°œì§€ì—­ ì•ˆì „ì§€ë„ â€” ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ & ìœ„í—˜ ë ˆì´ì–´")
st.markdown(
    "ì´ ì•±ì€ í•œêµ­ë„ë¡œêµí†µê³µë‹¨ì˜ ì‚¬ê³ ë‹¤ë°œì§€ì—­ CSVë¥¼ ì‚¬ìš©í•´ **ì‚¬ê³  ìœ„ì¹˜ í‘œì¶œ**, **íˆíŠ¸ë§µ/í´ëŸ¬ìŠ¤í„°**, "
    "**í•„í„°/ê²€ìƒ‰**, ê·¸ë¦¬ê³  **ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ(ë°ì´í„° ê¸°ë°˜)** ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n"
    ":warning: ì‹¤ì œ ë‚´ë¹„ê²Œì´ì…˜(ë„ë¡œ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ê²½ë¡œ)ì€ ì™¸ë¶€ ë¼ìš°íŒ… APIê°€ í•„ìš”í•©ë‹ˆë‹¤. "
    "ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë§Œìœ¼ë¡œ ê°€ëŠ¥í•œ ê·¼ì‚¬(ìœ„í—˜ íšŒí”¼) ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
)

# -------------------------
# ê¸°ë³¸ ì»¬ëŸ¼ ì²´í¬ ë° í‘œì¤€í™”
# -------------------------
has_latlon = {"ìœ„ë„", "ê²½ë„"}.issubset(set(data.columns))
has_year = "ì‚¬ê³ ì—°ë„" in data.columns or "ì—°ë„" in data.columns
year_col = "ì‚¬ê³ ì—°ë„" if "ì‚¬ê³ ì—°ë„" in data.columns else ("ì—°ë„" if "ì—°ë„" in data.columns else None)
type_col = "ì‚¬ê³ ìœ í˜•êµ¬ë¶„" if "ì‚¬ê³ ìœ í˜•êµ¬ë¶„" in data.columns else None
severity_related_cols = set(["ì‚¬ë§ììˆ˜", "ì¤‘ìƒììˆ˜", "ê²½ìƒììˆ˜", "ì‚¬ê³ ê±´ìˆ˜", "ì‚¬ìƒììˆ˜"]) & set(data.columns)

# -------------------------
# ì‚¬ì´ë“œë°”: í•„í„° & ê²€ìƒ‰
# -------------------------
st.sidebar.header("ğŸ” í•„í„° Â· ê²€ìƒ‰ Â· ì•ˆì „ê²½ë¡œ")

if year_col:
    years = sorted(data[year_col].dropna().unique().astype(int))
    sel_year = st.sidebar.slider("ì—°ë„ ì„ íƒ", min_value=int(min(years)), max_value=int(max(years)), value=int(max(years)))
else:
    sel_year = None

if type_col:
    types = sorted(data[type_col].dropna().unique())
    sel_types = st.sidebar.multiselect("ì‚¬ê³ ìœ í˜• í•„í„°", options=types, default=types)
else:
    sel_types = None

possible_cause_cols = [c for c in data.columns if "ì›ì¸" in c or "ì‚¬ê³ ì›ì¸" in c or "ë°œìƒì›ì¸" in c]
cause_col = possible_cause_cols[0] if possible_cause_cols else None
if cause_col:
    causes = sorted(data[cause_col].dropna().unique())
    sel_causes = st.sidebar.multiselect("ì‚¬ê³ ì›ì¸ í•„í„°", options=causes, default=causes)
else:
    sel_causes = None

date_col = None
for c in data.columns:
    if "ì¼ì" in c or "date" in c.lower() or "ë‚ ì§œ" in c:
        date_col = c
        break

if date_col:
    try:
        data[date_col + "_parsed"] = pd.to_datetime(data[date_col], errors="coerce")
        min_d = data[date_col + "_parsed"].min()
        max_d = data[date_col + "_parsed"].max()
        sel_dates = st.sidebar.date_input("ê¸°ê°„ í•„í„°", value=(max_d.date(), max_d.date()), min_value=min_d.date(), max_value=max_d.date())
    except Exception:
        date_col = None

search_text = st.sidebar.text_input("ê²€ìƒ‰(ì§€ì—­ëª… / ìœ„ì¹˜ì½”ë“œ / ìœ„ì¹˜ëª…) â€” ë¹ˆì¹¸=ì „ì²´", "")

loc_name_col = None
for c in ["ì‚¬ê³ ì§€ì—­ìœ„ì¹˜ëª…", "ì‚¬ê³ ë‹¤ë°œì§€ì—­ì‹œë„ì‹œêµ°êµ¬", "ìœ„ì¹˜ì½”ë“œ"]:
    if c in data.columns:
        loc_name_col = c
        break
loc_options = data[loc_name_col].astype(str).unique().tolist() if loc_name_col else []

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸš— ê·¼ì‚¬ ì•ˆì „ê²½ë¡œ(ë°ì´í„° ê¸°ë°˜)")
start_sel = st.sidebar.selectbox("ì¶œë°œì§€ (ë°ì´í„° ìœ„ì¹˜ ì¤‘ ì„ íƒ)", options=loc_options, index=0 if loc_options else -1)
end_sel = st.sidebar.selectbox("ë„ì°©ì§€ (ë°ì´í„° ìœ„ì¹˜ ì¤‘ ì„ íƒ)", options=loc_options, index=len(loc_options)-1 if loc_options else -1)
candidate_count = st.sidebar.slider("ê²½ë¡œ í›„ë³´ ìˆ˜", 3, 9, 5)
samples_per_candidate = st.sidebar.slider("ê²½ë¡œ ìƒ˜í”Œ ì  ê°œìˆ˜(ì •ë°€ë„)", 10, 80, 30)
avoid_radius_km = st.sidebar.slider("ìœ„í—˜ ê°€ì¤‘ ë°˜ê²½ (km)", 0.2, 3.0, 0.7)

st.sidebar.markdown("---")
st.sidebar.subheader("âš ï¸ ìœ„í—˜ êµ¬ê°„ ê²½ê³  (ì„ì˜ ìœ„ì¹˜)")
alert_lat = st.sidebar.number_input("ìœ„ë„ ì…ë ¥", value=float(data["ìœ„ë„"].mean()) if has_latlon else 37.56)
alert_lon = st.sidebar.number_input("ê²½ë„ ì…ë ¥", value=float(data["ê²½ë„"].mean()) if has_latlon else 126.97)
alert_radius_km = st.sidebar.slider("ê²½ê³  ë°˜ê²½ (km)", 0.1, 5.0, 0.5)

# -------------------------
# ë°ì´í„° í•„í„°ë§ ì ìš©
# -------------------------
df = data.copy()
if sel_year and year_col: df = df[df[year_col] == sel_year]
if sel_types and type_col: df = df[df[type_col].isin(sel_types)]
if sel_causes and cause_col: df = df[df[cause_col].isin(sel_causes)]
if date_col:
    start_d, end_d = sel_dates
    mask = (df[date_col + "_parsed"].dt.date >= start_d) & (df[date_col + "_parsed"].dt.date <= end_d)
    df = df[mask]
if search_text:
    search_text = search_text.strip()
    text_cols = [c for c in df.columns if df[c].dtype == object]
    mask = pd.Series(False, index=df.index)
    for c in text_cols:
        mask = mask | df[c].astype(str).str.contains(search_text, case=False, na=False)
    df = df[mask]

# -------------------------
# ì‹¬ê°ë„ ê³„ì‚°
# -------------------------
def severity_score(row):
    score = 0.0
    if "ì‚¬ë§ììˆ˜" in row.index: score += 10.0 * (row.get("ì‚¬ë§ììˆ˜", 0) or 0)
    if "ì¤‘ìƒììˆ˜" in row.index: score += 3.0 * (row.get("ì¤‘ìƒììˆ˜", 0) or 0)
    if "ê²½ìƒììˆ˜" in row.index: score += 1.0 * (row.get("ê²½ìƒììˆ˜", 0) or 0)
    if "ì‚¬ê³ ê±´ìˆ˜" in row.index: score += 0.5 * (row.get("ì‚¬ê³ ê±´ìˆ˜", 0) or 0)
    return score

df["sev_score"] = df.apply(severity_score, axis=1) if len(df) > 0 else []

def severity_to_color(s):
    if s >= 10: return [180, 0, 0, 200]
    elif s >= 5: return [230, 40, 40, 180]
    elif s >= 2: return [255, 140, 0, 150]
    elif s > 0: return [255, 210, 0, 130]
    else: return [150, 150, 150, 90]

df["color"] = df["sev_score"].apply(severity_to_color) if len(df) > 0 else []

# -------------------------
# ì§€ë„ ë ˆì´ì•„ì›ƒ
# -------------------------
st.subheader("ì§€ë„ Â· íˆíŠ¸ë§µ Â· í´ëŸ¬ìŠ¤í„° Â· ë§ˆì»¤")
if not has_latlon:
    st.error("ë°ì´í„°ì— 'ìœ„ë„' / 'ê²½ë„' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ íŒŒì¼ì— í•´ë‹¹ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    center_lat = float(df["ìœ„ë„"].mean()) if not np.isnan(df["ìœ„ë„"].mean()) else 37.56
    center_lon = float(df["ê²½ë„"].mean()) if not np.isnan(df["ê²½ë„"].mean()) else 126.97

    layers = []

    heat_layer = pdk.Layer(
        "HeatmapLayer",
        data=df,
        get_position=["ê²½ë„", "ìœ„ë„"],
        aggregation="SUM",
        weight="sev_score" if "sev_score" in df.columns else None,
        radiusPixels=60,
    )
    layers.append(heat_layer)

    hex_layer = pdk.Layer(
        "HexagonLayer",
        data=df,
        get_position=["ê²½ë„", "ìœ„ë„"],
        radius=200,
        elevation_scale=50,
        elevation_range=[0, 3000],
        pickable=True,
        extruded=True,
    )
    layers.append(hex_layer)

    scatter = pdk.Layer(
        "ScatterplotLayer",
        data=df,
        get_position=["ê²½ë„", "ìœ„ë„"],
        get_color="color",
        get_radius=60,
        pickable=True,
    )
    layers.append(scatter)

    view_state = pdk.ViewState(latitude=center_lat, longitude=center_lon, zoom=6, pitch=0)
    tooltip = {
        "html": "<b>{ì‚¬ê³ ì§€ì—­ìœ„ì¹˜ëª…}</b><br/>ì‚¬ê³ ê±´ìˆ˜: {ì‚¬ê³ ê±´ìˆ˜} / ì‚¬ìƒì: {ì‚¬ìƒììˆ˜} / ì‹¬ê°ë„:{sev_score}",
        "style": {"color": "white"}
    }
    deck = pdk.Deck(
        map_style="mapbox://styles/mapbox/light-v9",
        initial_view_state=view_state,
        layers=layers,
        tooltip=tooltip
    )
    st.pydeck_chart(deck, use_container_width=True)

# -------------------------
# ì´í•˜ í†µê³„, ìœ„í—˜ ê²½ê³ , ê·¼ì‚¬ ê²½ë¡œ, ë§ˆë¬´ë¦¬
# -------------------------
# ... ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì´ì–´ì„œ ì‚¬ìš© ...
